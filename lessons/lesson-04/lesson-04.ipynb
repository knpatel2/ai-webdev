{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>AI in Web Development</h1></center>\n",
        "\n",
        "---\n",
        "\n",
        "<center><h2>Lesson 04</h2></center>\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/snsie/ai-webdev/blob/main/lessons/lesson-04/lesson-04.ipynb)"
      ],
      "metadata": {
        "id": "IPgoWtoIJQOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on [this example](https://colab.research.google.com/github/ehennis/ReinforcementLearning/blob/master/05-DQN.ipynb#scrollTo=DPWjJiOZ2uVd) and [this example](https://colab.research.google.com/github/MrSyee/pg-is-all-you-need/blob/master/02.PPO.ipynb)\n",
        "\n"
      ],
      "metadata": {
        "id": "KPb9IjzQmKOU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf4_QpE72uVZ"
      },
      "source": [
        "<h1 align=\"center\">Reinforcement Learning (RL)</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/snsie/ai-webdev/blob/main/images/what-is-reinforcement-learning.png?raw=true\" width='320px'/></center>\n"
      ],
      "metadata": {
        "id": "VqGHxllEo3h7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Environment\n",
        "  * The stage that contains the simulation\n",
        "\n",
        "<br/>\n",
        "\n",
        "###Agent\n",
        "  * The entity making decisions\n",
        "  * Can be represented as a neural network\n",
        "\n",
        "<br/>\n",
        "\n",
        "###States\n",
        "* Set of observations that agents that can be performed by the agent\n",
        "* example: agent's position\n",
        "\n",
        "<br/>\n",
        "\n",
        "###Actions\n",
        "* Set of activities that can be performed by the agent\n",
        "* example: move right, move left\n",
        "\n",
        "<br/>\n",
        "\n",
        "###Rewards\n",
        "* Provides agents feedback about their performance\n"
      ],
      "metadata": {
        "id": "sdedL4816Qp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>Cart Pole Example</h1></center>\n",
        "\n",
        "---\n",
        "\n",
        "<center><h4>the cart's goal: balance the pole</h4></center>\n",
        "\n",
        "\n",
        "[Gym Docs](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)"
      ],
      "metadata": {
        "id": "P12W_YPa09UE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Bad Cart          |  Good Cart  |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "| <img src='https://github.com/snsie/ai-webdev/blob/main/images/cartpole-initial.gif?raw=true' width=\"300\"/>  | <img src='https://github.com/snsie/ai-webdev/blob/main/images/cartpole-trained.gif?raw=true' width=\"300\"/> |"
      ],
      "metadata": {
        "id": "A-YYa0w41cqt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Fqe9TO2uVd"
      },
      "source": [
        "<center><h3><u>Actions</u></h3></center>\n",
        "\n",
        "| Num | Action                 |\n",
        "|-----|------------------------|\n",
        "| 0   | Push cart to the left  |\n",
        "| 1   | Push cart to the right |\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "<center><h3><u>States</u></h3></center>\n",
        "\n",
        "| Num | Observation           | Min                 | Max               |\n",
        "|-----|-----------------------|:-------------------|:-----------------|\n",
        "| 0   | Cart Position         | -4.8                | 4.8               |\n",
        "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
        "| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
        "\n",
        "<br/>\n",
        "\n",
        "[Cart Pole Python File](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RL Algorithm to be used in this example:\n",
        "\n",
        "<center><h1>Q-learning</h1></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<h4>\n",
        "\\begin{align}\n",
        "Q(s,a) = r(s,a) + \\gamma \\cdot \\max_{a} Q(s',a')\n",
        "\\end{align}\n",
        "</h4>\n",
        "\n",
        "<br/>\n",
        "\n",
        "###$Q(s,a)$ = Q-value\n",
        "\n",
        "###$r(s,a)$ = reward for current action\n",
        "\n",
        "###$\\gamma$ = parameter that scales: $\\max_{a}Q(s',a')$\n",
        "\n",
        "###$\\max_{a}Q(s',a')$ = Maximum Q-value predicted in the next state \n",
        "\n",
        "<br/>"
      ],
      "metadata": {
        "id": "u1IqACZf-7yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from: https://colab.research.google.com/github/MrSyee/pg-is-all-you-need/blob/master/02.PPO.ipynb#scrollTo=weZ2GZJDIu1u\n",
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !apt install python-opengl\n",
        "    !apt install ffmpeg\n",
        "    !apt install xvfb\n",
        "    !pip install pyvirtualdisplay\n",
        "    from pyvirtualdisplay import Display\n",
        "    \n",
        "    # Start virtual display\n",
        "    dis = Display(visible=0, size=(600, 400))\n",
        "    dis.start()"
      ],
      "metadata": {
        "id": "KtRw868MHSEj",
        "outputId": "54993c0e-00a1-4d9e-8fee-df176eb07937",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.11).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "DPWjJiOZ2uVd",
        "outputId": "881436ac-d489-44d1-bc67-5a176656a705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "#Imports\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from typing import Deque, Dict, List, Tuple\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.optimizers import Adam\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "#Create Gym\n",
        "from gym import wrappers\n",
        "envCartPole = gym.make('CartPole-v1')\n",
        "envCartPole.seed(50) #Set the seed to keep the environment consistent across runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A9R6ybk2uVg"
      },
      "source": [
        "**Experience Replay**  \n",
        "To perform an experience replay, the algorithm stores all of the agents experiences {$s_t,a_t,r_t,s_{t+1}$} at each time step in a data set. Normally in a q-learner, we would run the update rule on them. But, with experience replay we just store them.  \n",
        "\n",
        "Later during the training process these replays will be drawn uniformly from the memory queue and be ran through the update rule. There are 2 ways to handle this and I have coded both in the past. The first is to run them on every loop and the other is to run them after X amount of runs. In this code below, I run them each time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsbhTZLQ2uVo"
      },
      "source": [
        "**Deep Q-Network Class**  \n",
        "The following class is the deep Q-network that is built using the neural network code from Keras.  \n",
        "**init**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This creates the class and sets the local parameters.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I use a *deque* for the local memory to hold the experiences and a keras model for the NN.  \n",
        "\n",
        "**build_model(self)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This builds the NN. I am using sequential model. Each of the layers are *Dense* despite the fact the document talks about using *Convolution*. But, they are only using that because they need to convert pixels and I already have numbers.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I am using an input layer(4), 24 neuron layer, 24 neuron layer, and an output layer(2).  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For calculating the loss I am using mean squared error.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For an optimizer I am using [Adam](https://arxiv.org/abs/1412.6980v8). It is a variant of gradient descent and you can read the technical document at the link. If you want a slightly lighter explaining you can check out [Machine Learning Mastery](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/). You could also use SGD (Stochastic Gradient Descent) but Adam gives me better results and seems to be the standard in most examples.  \n",
        "\n",
        "**action(self,state):**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This generates the action.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Explore: I am using the epsilon like previous lessons.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Exploit: I use the NN to grab the 2 possible actions and then grab the argmax to find the better one  \n",
        "\n",
        "**test_action(self,state):**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This generates the action when I am testing. I want to 100% exploit  \n",
        "\n",
        "**store(self, state, action, reward, nstate, done):**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This places the observables in memory  \n",
        "\n",
        "**experience_replay(self, batch_size):**  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is where the training occurs. We grab the sample batches and then use the NN to predict the optimal action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "7CsaVrbA2uVp"
      },
      "outputs": [],
      "source": [
        "class DeepQNetwork():\n",
        "    def __init__(self, states, actions, learningRate, gamma, epsilonInit,epsilon_min, epsilon_decay):\n",
        "        self._numStates = states\n",
        "        self._numActions = actions\n",
        "        self.memory = deque([], maxlen=2500)\n",
        "        self._learningRate = learningRate\n",
        "        self._gamma = gamma\n",
        "        #Explore/Exploit\n",
        "        self.epsilon = epsilonInit\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.model = self.build_model()\n",
        "        self.loss = []\n",
        "    \n",
        "    # create a function to initialize the neural network\n",
        "    def build_model(self):\n",
        "        model = keras.Sequential() #linear stack of layers https://keras.io/models/sequential/\n",
        "        model.add(keras.layers.Dense(64, input_dim=self._numStates, activation='relu')) #[Input] -> Layer 1\n",
        "        # model.add(keras.layers.Dense(64, activation='relu')) #Layer 2 -> 3\n",
        "        model.add(keras.layers.Dense(self._numActions, activation='linear')) #Layer 3 -> [output]\n",
        "        #   Size has to match the output (different actions)\n",
        "        #   Linear activation on the last layer\n",
        "        model.compile(loss='mean_squared_error', #Loss function: Mean Squared Error\n",
        "                      optimizer=keras.optimizers.Adam(learning_rate=self._learningRate)) #Optimaizer: Adam (Feel free to check other options)\n",
        "        return model\n",
        "\n",
        "    def action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self._numActions) #Explore\n",
        "        action_vals = self.model.predict(state) #Exploit: Use the NN to predict the correct action from this state\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def test_action(self, state): #Exploit\n",
        "        action_vals = self.model.predict(state)\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def store(self, state, action, reward, nstate, done):\n",
        "        #Store the experience in memory\n",
        "        self.memory.append( (state, action, reward, nstate, done) )\n",
        "\n",
        "    def experience_replay(self, batch_size):\n",
        "        #Execute the experience replay\n",
        "        minibatch = random.sample( self.memory, batch_size ) #Randomly sample from memory\n",
        "        #Convert to numpy for speed by vectorization\n",
        "        x = []\n",
        "        y = []\n",
        "        np_array = np.array(minibatch)\n",
        "        # print(np_array)\n",
        "        st = np.zeros((0,self._numStates)) #States\n",
        "        nst = np.zeros( (0,self._numStates) )#Next States\n",
        "        for i in range(len(np_array)): #Creating the state and next state np arrays\n",
        "            st = np.append( st, np_array[i,0], axis=0)\n",
        "            nst = np.append( nst, np_array[i,3], axis=0)\n",
        "        st_predict = self.model.predict(st) #Here is the speedup! I can predict on the ENTIRE batch\n",
        "        nst_predict = self.model.predict(nst)\n",
        "        index = 0\n",
        "        targetArr=[]\n",
        "        for state, action, reward, nstate, done in minibatch:\n",
        "            x.append(state)\n",
        "            #Predict from state\n",
        "            nst_action_predict_model = nst_predict[index]\n",
        "            if done == True: #Terminal: Just assign reward much like {* (not done) - QB[state][action]}\n",
        "                target = reward\n",
        "                # targetArr.append(target)\n",
        "                # self.targetAve+=target\n",
        "            else:   #Non terminal\n",
        "                # target = np.min([1,reward + self._gamma * np.amax(nst_action_predict_model)])\n",
        "                target = reward + self._gamma * np.amax(nst_action_predict_model)\n",
        "                # targetArr.append(target)\n",
        "                # self.targetAve+=target\n",
        "            # print(targetArr)            \n",
        "            target_f = st_predict[index]\n",
        "            target_f[action] = target\n",
        "            y.append(target_f)\n",
        "            index += 1\n",
        "        #Reshape for Keras Fit\n",
        "        x_reshape = np.array(x).reshape(batch_size,self._numStates)\n",
        "        y_reshape = np.array(y)\n",
        "        epoch_count = 1 #Epochs is the number or iterations\n",
        "        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n",
        "        #Graph Losses\n",
        "        for i in range(epoch_count):\n",
        "            self.loss.append( hist.history['loss'][i] )\n",
        "        #Decay Epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plotTotalReward(episodeNum, tot_rewards, epsilon,storedRewards,storedEpsilons):\n",
        "    def subplot(loc: int, title: str, values: List[float]):\n",
        "        plt.subplot(loc)\n",
        "        plt.title(title)\n",
        "        plt.plot(values)\n",
        "\n",
        "    subplot_params = [\n",
        "        (121, f\"episode: {episodeNum}, total reward: {tot_rewards}\", storedRewards),\n",
        "        (122, f\"episode: {episodeNum}, epsilon: {epsilon}\", storedEpsilons),\n",
        "    ]\n",
        "\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(30, 5))\n",
        "    for loc, title, values in subplot_params:\n",
        "        subplot(loc, title, values)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KVleneWxZqGq"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "EomjmDOb2uVn"
      },
      "outputs": [],
      "source": [
        "#Global Variables\n",
        "EPISODES = 500\n",
        "TRAIN_END = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "yianM-dx2uVn"
      },
      "outputs": [],
      "source": [
        "#Hyper Parameters\n",
        "\n",
        "gamma=0.95\n",
        "\n",
        "learningRate=0.1\n",
        "\n",
        "batchSize=32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "azVNSMYa2uVq"
      },
      "outputs": [],
      "source": [
        "#Create the agent\n",
        "nS = envCartPole.observation_space.shape[0] #This is only 4\n",
        "nA = envCartPole.action_space.n #Actions\n",
        "dqn = DeepQNetwork(nS, nA, learningRate, gamma, 0, 0.001, 0.995 )\n",
        "batch_size = batchSize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "uFZuVbZz2uVq",
        "outputId": "0a2cff06-1c5b-4454-8b07-2e8092a41821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2160x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-6de791a6d68f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrainAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-83-6de791a6d68f>\u001b[0m in \u001b[0;36mtrainAgent\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0;31m#Experience Replay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m               \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0;31m#If our current NN passes we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;31m#I am going to use the last 5 runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-6acd8fa6fcfe>\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mnst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mst_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Here is the speedup! I can predict on the ENTIRE batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mnst_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1976\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1978\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1979\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 755\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3314\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3315\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3316\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3318\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Training\n",
        "def trainAgent():\n",
        "  rewards = [] #Store rewards for graphing\n",
        "  epsilons = [] # Store the Explore/Exploit\n",
        "  TEST_Episodes = 0\n",
        "  for e in range(EPISODES):\n",
        "      state = envCartPole.reset()\n",
        "      state = np.reshape(state, [1, nS]) # Resize to store in memory to pass to .predict\n",
        "      tot_rewards = 0\n",
        "      for time in range(210): #200 is when you \"solve\" the game. This can continue forever as far as I know\n",
        "          action = dqn.action(state)\n",
        "          nstate, reward, done, _ = envCartPole.step(action)\n",
        "          nstate = np.reshape(nstate, [1, nS])\n",
        "          tot_rewards += reward\n",
        "          dqn.store(state, action, reward, nstate, done) # Resize to store in memory to pass to .predict\n",
        "          state = nstate\n",
        "          #done: CartPole fell. \n",
        "          #time == 209: CartPole stayed upright\n",
        "          if done or time == 209:\n",
        "              rewards.append(tot_rewards)\n",
        "              epsilons.append(dqn.epsilon)\n",
        "              plotTotalReward(e, tot_rewards, dqn.epsilon,rewards,epsilons)\n",
        "              # print(\"episode: {}/{}, score: {}, e: {}\"\n",
        "              #       .format(e, EPISODES, tot_rewards, dqn.epsilon))\n",
        "              break\n",
        "          #Experience Replay\n",
        "          if len(dqn.memory) > batch_size:\n",
        "              dqn.experience_replay(batch_size)\n",
        "      #If our current NN passes we are done\n",
        "      #I am going to use the last 5 runs\n",
        "      if len(rewards) > 5 and np.average(rewards[-5:]) > 195:\n",
        "          #Set the rest of the EPISODES for testing\n",
        "          TEST_Episodes = EPISODES - e\n",
        "          TRAIN_END = e\n",
        "          break\n",
        "\n",
        "trainAgent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0hDY0PZ2uVr"
      },
      "outputs": [],
      "source": [
        "#Test the agent that was trained\n",
        "#   In this section we ALWAYS use exploit don't train any more\n",
        "for e_test in range(TEST_Episodes):\n",
        "    state = envCartPole.reset()\n",
        "    state = np.reshape(state, [1, nS])\n",
        "    tot_rewards = 0\n",
        "    for t_test in range(210):\n",
        "        action = dqn.test_action(state)\n",
        "        nstate, reward, done, _ = envCartPole.step(action)\n",
        "        nstate = np.reshape( nstate, [1, nS])\n",
        "        tot_rewards += reward\n",
        "        #DON'T STORE ANYTHING DURING TESTING\n",
        "        state = nstate\n",
        "        #done: CartPole fell. \n",
        "        #t_test == 209: CartPole stayed upright\n",
        "        if done or t_test == 209: \n",
        "            rewards.append(tot_rewards)\n",
        "            epsilons.append(0) #We are doing full exploit\n",
        "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
        "                  .format(e_test, TEST_Episodes, tot_rewards, 0))\n",
        "            break;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJyZYjHf2uVr"
      },
      "source": [
        "**Results**  \n",
        "Here is a graph of the results. If everything was done correctly you should see the rewards over the red line.  \n",
        "\n",
        "Black: This is the 100 episode rolling average  \n",
        "Red: This is the \"solved\" line at 195  \n",
        "Blue: This is the reward for each episode  \n",
        "Green: This is the value of epsilon scaled by 200  \n",
        "Yellow: This is where the tests started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C265YboP2uVs"
      },
      "outputs": [],
      "source": [
        "rolling_average = np.convolve(rewards, np.ones(100)/100)\n",
        "\n",
        "plt.plot(rewards)\n",
        "plt.plot(rolling_average, color='black')\n",
        "plt.axhline(y=195, color='r', linestyle='-') #Solved Line\n",
        "#Scale Epsilon (0.001 - 1.0) to match reward (0 - 200) range\n",
        "eps_graph = [200*x for x in epsilons]\n",
        "plt.plot(eps_graph, color='g', linestyle='-')\n",
        "#Plot the line where TESTING begins\n",
        "plt.axvline(x=TRAIN_END, color='y', linestyle='-')\n",
        "plt.xlim( (0,EPISODES) )\n",
        "plt.ylim( (0,220) )\n",
        "plt.show()\n",
        "\n",
        "\n",
        "envCartPole.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # test\n",
        "# if IN_COLAB:\n",
        "#     agent.env = gym.wrappers.Monitor(agent.env, \"videos\", force=True)\n",
        "# frames = agent.test()"
      ],
      "metadata": {
        "id": "KgtBDepYHtTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from: https://colab.research.google.com/github/MrSyee/pg-is-all-you-need/blob/master/02.PPO.ipynb#scrollTo=_9SokMoDIu18\n",
        "if IN_COLAB:  # for colab\n",
        "    import base64\n",
        "    import glob\n",
        "    import io\n",
        "    import os\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    def ipython_show_video(path: str) -> None:\n",
        "        \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
        "        if not os.path.isfile(path):\n",
        "            raise NameError(\"Cannot access: {}\".format(path))\n",
        "\n",
        "        video = io.open(path, \"r+b\").read()\n",
        "        encoded = base64.b64encode(video)\n",
        "\n",
        "        display(HTML(\n",
        "            data=\"\"\"\n",
        "            <video alt=\"test\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "            </video>\n",
        "            \"\"\".format(encoded.decode(\"ascii\"))\n",
        "        ))\n",
        "\n",
        "    list_of_files = glob.glob(\"videos/*.mp4\")\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(latest_file)\n",
        "    ipython_show_video(latest_file)\n",
        "\n",
        "else:  # for jupyter\n",
        "    from matplotlib import animation\n",
        "    from JSAnimation.IPython_display import display_animation\n",
        "    from IPython.display import display\n",
        "\n",
        "\n",
        "    def display_frames_as_gif(frames):\n",
        "        \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
        "        patch = plt.imshow(frames[0])\n",
        "        plt.axis('off')\n",
        "\n",
        "        def animate(i):\n",
        "            patch.set_data(frames[i])\n",
        "\n",
        "        anim = animation.FuncAnimation(\n",
        "            plt.gcf(), animate, frames = len(frames), interval=50\n",
        "        )\n",
        "        display(display_animation(anim, default_mode='loop'))\n",
        "\n",
        "\n",
        "    # display \n",
        "    display_frames_as_gif(frames)"
      ],
      "metadata": {
        "id": "oWEs0GSxG8Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmbHdtuP2uVs"
      },
      "source": [
        "**Changes**  \n",
        "*hyper parameters*: You can alter alpha, gamma, batch size, and episode length to see what differences the algorithm returns.  \n",
        "*Training End*: You can also change the line where I only check the last 5 runs before switching to testing mode (if len(rewards) > 5 and np.average(rewards[-5:]) > 195:) as that doesn't prove it was solved. The reason I did this was because I wanted to limit the amount of runs I made.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yExWip-F2uVs"
      },
      "source": [
        "**Conclusion**  \n",
        "This is a Deep Q-Network implementation. There are some changes you can make here and there but it follows the paper. Hopefully, you were able to understand the code as well as make your own version to compare with this one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "YNoagP352uVt"
      },
      "source": [
        "**Reference**  \n",
        "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). *Human-level control through deep reinforcement learning*. Nature, 518(7540), 529"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "lesson-04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}