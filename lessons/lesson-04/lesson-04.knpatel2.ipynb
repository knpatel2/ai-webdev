{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>AI in Web Development</h1></center>\n",
        "\n",
        "---\n",
        "\n",
        "<center><h2>Lesson 04</h2></center>\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/knpatel2/ai-webdev/blob/main/lessons/lesson-04/lesson-04.knpatel2.ipynb)"
      ],
      "metadata": {
        "id": "IPgoWtoIJQOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on [this example](https://colab.research.google.com/github/Curt-Park/rainbow-is-all-you-need/blob/master/01.dqn.ipynb)\n",
        "and [this example](https://colab.research.google.com/github/ehennis/ReinforcementLearning/blob/master/05-DQN.ipynb#scrollTo=DPWjJiOZ2uVd)  \n"
      ],
      "metadata": {
        "id": "KPb9IjzQmKOU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf4_QpE72uVZ"
      },
      "source": [
        "<h1 align=\"center\">Reinforcement Learning (RL)</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/snsie/ai-webdev/blob/main/images/what-is-reinforcement-learning.png?raw=true\" width='320px'/></center>\n"
      ],
      "metadata": {
        "id": "VqGHxllEo3h7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Environment\n",
        "  * The stage that contains the simulation\n",
        "\n",
        "<br/>\n",
        "\n",
        "###Agent\n",
        "  * The entity making decisions\n",
        "  * Can be represented as a neural network\n",
        "\n",
        "<br/>\n",
        "\n",
        "###States\n",
        "* Set of observations that agents that can be performed by the agent\n",
        "* example: agent's position\n",
        "\n",
        "<br/>\n",
        "\n",
        "###Actions\n",
        "* Set of activities that can be performed by the agent\n",
        "* example: move right, move left\n",
        "\n",
        "<br/>\n",
        "\n",
        "###Rewards\n",
        "* Provides agents feedback about their performance\n"
      ],
      "metadata": {
        "id": "sdedL4816Qp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>Cart Pole Example</h1></center>\n",
        "\n",
        "---\n",
        "\n",
        "<center><h4>the cart's goal: balance the pole</h4></center>\n",
        "\n",
        "\n",
        "[Gym Docs](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)"
      ],
      "metadata": {
        "id": "P12W_YPa09UE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|Bad Cart          |  Good Cart  |\n",
        "|:-------------------------:|:-------------------------:|\n",
        "| <img src='https://github.com/snsie/ai-webdev/blob/main/images/cartpole-initial.gif?raw=true' width=\"300\"/>  | <img src='https://github.com/snsie/ai-webdev/blob/main/images/cartpole-trained.gif?raw=true' width=\"300\"/> |"
      ],
      "metadata": {
        "id": "A-YYa0w41cqt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Fqe9TO2uVd"
      },
      "source": [
        "<center><h3><u>Actions</u></h3></center>\n",
        "\n",
        "| Num | Action                 |\n",
        "|-----|------------------------|\n",
        "| 0   | Push cart to the left  |\n",
        "| 1   | Push cart to the right |\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "<center><h3><u>States</u></h3></center>\n",
        "\n",
        "| Num | Observation           | Min                 | Max               |\n",
        "|-----|-----------------------|:-------------------|:-----------------|\n",
        "| 0   | Cart Position         | -4.8                | 4.8               |\n",
        "| 1   | Cart Velocity         | -Inf                | Inf               |\n",
        "| 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
        "| 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
        "\n",
        "<br/>\n",
        "\n",
        "[Cart Pole Python File](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RL Algorithm to be used in this example:\n",
        "\n",
        "<center><h1>Q-learning</h1></center>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<h4>\n",
        "\\begin{align}\n",
        "Q(s,a) = r(s,a) + \\gamma \\cdot \\max_{a} Q(s',a')\n",
        "\\end{align}\n",
        "</h4>\n",
        "\n",
        "<br/>\n",
        "\n",
        "###$Q(s,a)$ = Q-value\n",
        "\n",
        "###$r(s,a)$ = reward for current action\n",
        "\n",
        "###$\\gamma$ = parameter that scales: $\\max_{a}Q(s',a')$\n",
        "\n",
        "###$\\max_{a}Q(s',a')$ = Maximum Q-value predicted in the next state \n",
        "\n",
        "<br/>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "u1IqACZf-7yG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Long term interpretation of Q:</h3>\n",
        "<h3>\n",
        "$$\n",
        "Q(s_t,a_t) = r(t) + \\gamma \\cdot r(t+1) + \\gamma^2 \\cdot r(t+2) +  \\gamma^3 \\cdot r(t+3) \\ ...\n",
        "$$\n",
        "</h3>"
      ],
      "metadata": {
        "id": "EtMh6rUaKpep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reference**  \n",
        "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). *Human-level control through deep reinforcement learning*. Nature, 518(7540), 529"
      ],
      "metadata": {
        "id": "vJM_vfH8gSf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !apt install python-opengl\n",
        "    !apt install ffmpeg\n",
        "    !apt install xvfb\n",
        "    !pip install PyVirtualDisplay==3.0\n",
        "    !pip install gym==0.21.0\n",
        "    from pyvirtualdisplay import Display\n",
        "    \n",
        "    # Start virtual display\n",
        "    dis = Display(visible=0, size=(400, 400))\n",
        "    dis.start()"
      ],
      "metadata": {
        "id": "KtRw868MHSEj",
        "outputId": "033fb55e-9953-417d-b2a9-248c5c71a155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.11).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyVirtualDisplay==3.0 in /usr/local/lib/python3.7/dist-packages (3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.21.0 in /usr/local/lib/python3.7/dist-packages (0.21.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21.0) (4.12.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.21.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.21.0) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym==0.21.0) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym==0.21.0) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DPWjJiOZ2uVd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output,HTML, display\n",
        "import base64\n",
        "import glob\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network\n",
        "\n",
        "We are going to use a simple network architecture with three fully connected layers and two non-linearity functions (ReLU)."
      ],
      "metadata": {
        "id": "hyKWUadCZASY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a neural network class \n",
        "class Network(nn.Module):\n",
        "  def __init__(self, in_dim:int, out_dim:int, num_hn:int):\n",
        "    super(Network, self).__init__()\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(in_dim, num_hn),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(num_hn, num_hn),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(num_hn, out_dim),\n",
        "    )\n",
        "\n",
        "  def forward(self, x:torch.Tensor):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "KVleneWxZqGq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay buffer\n",
        "\n",
        "Typically, people implement replay buffers with one of the following three data structures:\n",
        "\n",
        "  - collections.deque\n",
        "  - list\n",
        "  - numpy.ndarray\n",
        "  \n",
        "**deque** is very easy to handle once you initialize its maximum length (e.g. deque(maxlen=buffer_size)). However, the indexing operation of deque gets terribly slow as it grows up because it is [internally doubly linked list](https://wiki.python.org/moin/TimeComplexity#collections.deque). On the other hands, **list** is an array, so it is relatively faster than deque when you sample batches at every step. Its amortized cost of  *Get item* is [O(1)](https://wiki.python.org/moin/TimeComplexity#list).\n",
        "\n",
        "Last but not least, let's see **numpy.ndarray**. numpy.ndarray is even faster than list due to the fact that it is [a homogeneous array of fixed-size items](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray), so you can get the benefits of [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference). Whereas list is an array of pointers to objects, even when all of them are of the same type.\n",
        "\n",
        "Here, we are going to implement a replay buffer using numpy.ndarray.\n",
        "\n",
        "\n",
        "Reference: [OpenAI spinning-up](https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py#L10)"
      ],
      "metadata": {
        "id": "rj0SrJzkY5xY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7CsaVrbA2uVp"
      },
      "outputs": [],
      "source": [
        "#Create ReplayBuffer Class\n",
        "class ReplayBuffer:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN Agent\n",
        "\n",
        "Here is a summary of DQNAgent class.\n",
        "\n",
        "| Method           | Note                                                 |\n",
        "| ---              | ---                                                  |\n",
        "|select_action     | select an action from the input state.               |\n",
        "|step              | take an action and return the response of the env.   |\n",
        "|compute_dqn_loss  | return dqn loss.                                     |\n",
        "|update_model      | update the model by gradient descent.                |\n",
        "|target_hard_update| hard update from the local model to the target model.|\n",
        "|train             | train the agent during num_frames.                   |\n",
        "|test              | test the agent (1 episode).                          |\n",
        "|plot              | plot the training progresses.                        |\n"
      ],
      "metadata": {
        "id": "dx9OyOhCZKL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DQNAgent class \n",
        "class DQNAgent:\n",
        "    \"\"\"DQN Agent interacting with environment.\n",
        "    \n",
        "    Attribute:\n",
        "        env (gym.Env): openAI Gym environment\n",
        "        memory (ReplayBuffer): replay memory to store transitions\n",
        "        batch_size (int): batch size for sampling\n",
        "        epsilon (float): parameter for epsilon greedy policy\n",
        "        epsilon_decay (float): step size to decrease epsilon\n",
        "        max_epsilon (float): max value of epsilon\n",
        "        min_epsilon (float): min value of epsilon\n",
        "        target_update (int): period for target model's hard update\n",
        "        gamma (float): discount factor\n",
        "        dqn (Network): model to train and select actions\n",
        "        dqn_target (Network): target model to update\n",
        "        optimizer (torch.optim): optimizer for training dqn\n",
        "        transition (list): transition information including \n",
        "                           state, action, reward, next_state, done\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        env: gym.Env,\n",
        "        num_hidden_nodes:int,\n",
        "        memory_size: int,\n",
        "        batch_size: int,\n",
        "        target_update: int,\n",
        "        epsilon_decay: float,\n",
        "        max_epsilon: float = 1.0,\n",
        "        min_epsilon: float = 0.1,\n",
        "        gamma: float = 0.99,\n",
        "    ):\n",
        "        \"\"\"Initialization.\n",
        "        \n",
        "        Args:\n",
        "            env (gym.Env): openAI Gym environment\n",
        "            memory_size (int): length of memory\n",
        "            batch_size (int): batch size for sampling\n",
        "            target_update (int): period for target model's hard update\n",
        "            epsilon_decay (float): step size to decrease epsilon\n",
        "            lr (float): learning rate\n",
        "            max_epsilon (float): max value of epsilon\n",
        "            min_epsilon (float): min value of epsilon\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.n\n",
        "        \n",
        "        self.env = env\n",
        "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon = max_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.target_update = target_update\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        print(self.device)\n",
        "\n",
        "        # networks: dqn, dqn_target\n",
        "        self.dqn = Network(obs_dim, action_dim,num_hidden_nodes).to(self.device)\n",
        "        self.dqn_target = Network(obs_dim, action_dim,num_hidden_nodes).to(self.device)\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "        self.dqn_target.eval()\n",
        "        print('params',self.dqn.parameters())\n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam(self.dqn.parameters(),lr=0.001)\n",
        "\n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "        \n",
        "        # mode: train / test\n",
        "        self.is_test = False\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        # epsilon greedy policy\n",
        "        if self.epsilon > np.random.random():\n",
        "            selected_action = self.env.action_space.sample()\n",
        "        else:\n",
        "            selected_action = self.dqn(\n",
        "                torch.FloatTensor(state).to(self.device)\n",
        "            ).argmax()\n",
        "            selected_action = selected_action.detach().cpu().numpy()\n",
        "        \n",
        "        if not self.is_test:\n",
        "            self.transition = [state, selected_action]\n",
        "        \n",
        "        return selected_action\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        if not self.is_test:\n",
        "            self.transition += [reward, next_state, done]\n",
        "            self.memory.store(*self.transition)\n",
        "    \n",
        "        return next_state, reward, done\n",
        "\n",
        "    def update_model(self) -> torch.Tensor:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        samples = self.memory.sample_batch()\n",
        "\n",
        "        loss = self._compute_dqn_loss(samples)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "        \n",
        "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        self.is_test = False\n",
        "        \n",
        "        state = self.env.reset()\n",
        "        update_cnt = 0\n",
        "        epsilons = []\n",
        "        losses = []\n",
        "        scores = []\n",
        "        score = 0\n",
        "\n",
        "        for frame_idx in range(1, num_frames + 1):\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # if episode ends\n",
        "            if done:\n",
        "                state = self.env.reset()\n",
        "                scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "            # if training is ready\n",
        "            if len(self.memory) >= self.batch_size:\n",
        "                loss = self.update_model()\n",
        "                losses.append(loss)\n",
        "                update_cnt += 1\n",
        "                \n",
        "                # linearly decrease epsilon\n",
        "                self.epsilon = max(\n",
        "                    self.min_epsilon, self.epsilon - (\n",
        "                        self.max_epsilon - self.min_epsilon\n",
        "                    ) * self.epsilon_decay\n",
        "                )\n",
        "                epsilons.append(self.epsilon)\n",
        "                \n",
        "                # if hard update is needed\n",
        "                if update_cnt % self.target_update == 0:\n",
        "                    self._target_hard_update()\n",
        "\n",
        "            # plotting\n",
        "            if frame_idx % plotting_interval == 0:\n",
        "                self._plot(frame_idx, scores, losses, epsilons)\n",
        "                \n",
        "        self.env.close()\n",
        "                \n",
        "    def test(self, video_folder: str) -> None:\n",
        "        \"\"\"Test the agent.\"\"\"\n",
        "        self.is_test = True\n",
        "        \n",
        "        # for recording a video\n",
        "        naive_env = self.env\n",
        "        self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder)\n",
        "        \n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        \n",
        "        while not done:\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "        \n",
        "        print(\"score: \", score)\n",
        "        self.env.close()\n",
        "        \n",
        "        # reset\n",
        "        self.env = naive_env\n",
        "\n",
        "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray]) -> torch.Tensor:\n",
        "        \"\"\"Return dqn loss.\"\"\"\n",
        "        device = self.device  # for shortening the following lines\n",
        "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "        action = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
        "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "\n",
        "        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n",
        "        #       = r                       otherwise\n",
        "        curr_q_value = self.dqn(state).gather(1, action)\n",
        "        next_q_value = self.dqn_target(\n",
        "            next_state\n",
        "        ).max(dim=1, keepdim=True)[0].detach()\n",
        "        mask = 1 - done\n",
        "        target = (reward + self.gamma * next_q_value * mask).to(self.device)\n",
        "        # print(target)\n",
        "        # calculate dqn loss\n",
        "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _target_hard_update(self):\n",
        "        \"\"\"Hard update: target <- local.\"\"\"\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "                \n",
        "    def _plot(\n",
        "        self, \n",
        "        frame_idx: int, \n",
        "        scores: List[float], \n",
        "        losses: List[float], \n",
        "        epsilons: List[float],\n",
        "    ):\n",
        "        \"\"\"Plot the training progresses.\"\"\"\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(20, 5))\n",
        "        plt.subplot(131)\n",
        "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
        "        plt.plot(scores)\n",
        "        plt.subplot(132)\n",
        "        plt.title('loss')\n",
        "        plt.plot(losses)\n",
        "        plt.subplot(133)\n",
        "        plt.title('epsilons')\n",
        "        plt.plot(epsilons)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "YVbtpCYqZGv7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the environment"
      ],
      "metadata": {
        "id": "if11YIXAf_dG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFZuVbZz2uVq"
      },
      "outputs": [],
      "source": [
        "# set the environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set parameters for experiment"
      ],
      "metadata": {
        "id": "8yoJpvcXgCEL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yianM-dx2uVn"
      },
      "outputs": [],
      "source": [
        "# create params\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the agent using the DQN class\n"
      ],
      "metadata": {
        "id": "VUt0odA2kuSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "BPu85nk2ZfJa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azVNSMYa2uVq"
      },
      "outputs": [],
      "source": [
        "# train the agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test the network and export video of agent's actions"
      ],
      "metadata": {
        "id": "oUsgqvzyZh8q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EomjmDOb2uVn"
      },
      "outputs": [],
      "source": [
        "# create the video_folder var\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create the functions to display the test video in colab\n"
      ],
      "metadata": {
        "id": "sTQ7xo9DZUKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zJM-hU0bjg3Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "lesson-04.knpatel2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}